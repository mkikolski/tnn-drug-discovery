# Example configuration for molecule generation and RL training

[general]
# Path to the dataset directory (should contain SMILES files)
data_path = "/dataset/general"
# Maximum length of SMILES sequences
data_max_len = 100
# Device to use for training ("cuda" or "cpu")
device = "cuda"

[training]
# Learning rate for optimizer
learning_rate = 1e-3
# Number of pretraining epochs for the generator
pretraining_epochs = 20000
# Batch size for dataloaders
batch_size = 128
# Directory to save checkpoints
checkpoint_dir = "checkpoints"
# Save checkpoint every N epochs
save_frequency = 1
# Path to checkpoint to resume training from (set to empty string if not resuming)
resume_from = ""

[model]
# Hidden size for TNN and DQN
hidden_size = 256
# Number of transformer layers in TNN
n_layers = 6
# Number of attention heads in TNN
nheads = 8

[rl]
# Replay buffer capacity
buffer_capacity = 10000
# Batch size for DQN updates
rl_batch_size = 64
# Discount factor for RL
gamma = 0.99
# Learning rate for DQN
rl_learning_rate = 1e-3
# Path to CSV file for prepopulating replay buffer (SMILES, qed, sa, docking)
replay_csv = "replay_data.csv"
# Epsilon for epsilon-greedy policy
epsilon = 0.1

[generate]
# Number of molecules to generate
num_molecules = 100
# Sampling temperature
temperature = 0.7

[reward]
# QVina2 GPU executable
qvina2_gpu_path = "qv2gpu"
# OpenBabel executable
openbabel_path = "obabel"
# Path to the receptor PDB file
receptor_path = "receptor.pdb"
# Path to the output directory
output_dir = "output"
# Pocket coordinates
x = 11.443
y = -61.362
z = -6.894
# Pocket size
x_size = 30.0
y_size = 30.0
z_size = 30.0
# Number of threads - 8000 translates to classical vina exhaustiveness of about 128
threads = 8000
# Number of pretraining molecules to use for reward calculation
pretraining_molecules = 30000

